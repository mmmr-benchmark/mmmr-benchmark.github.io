# MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks

![MMMR Logo](static/images/MMMR_logo.ico)

## ğŸŒŸ Introduction

**MMMR** is a benchmarking framework designed to evaluate the reasoning capabilities of Multi-Modal Large Language Models (MLLMs) and their thinking-enhanced counterparts (MLLMs-T). It supports a wide range of task types involving multi-modal data (text, image, structured data), with a focus on:

- Logical Reasoning
- Mathematical Reasoning
- Spatio-Temporal Understanding
- Code Comprehension
- Map Navigation
- Scientific Reasoning

Visit the website [here](#) for a detailed overview.

## ğŸ“¦ Project Structure

.
â”œâ”€â”€ static/
â”‚ â”œâ”€â”€ css/ # CSS styles (Bulma, FontAwesome, etc.)
â”‚ â”œâ”€â”€ images/ # Icons, banners, and visual assets
â”œâ”€â”€ index.html # Main webpage


## ğŸ”— Live Resources

- ğŸ¤— [Dataset on HuggingFace](https://huggingface.co/datasets/csegirl/MMMR)  
- ğŸ’» [Code on GitHub](https://github.com/CsEgir/MLRM-Bench/tree/master)  
<!-- - ğŸ“„ [Paper on arXiv](https://arxiv.org/abs/<ARXIV_ID>) -->

## ğŸ§ª Highlights

- ğŸ“Š Bar charts comparing different MLLMs
- ğŸ§  Thinking Quality Assessment using RTQ, RTA, RSC, Acc, TLen, and OA
- ğŸ§© Insight synthesis to guide model improvement

## ğŸ‘¨â€ğŸ’» Authors

- Guiyao Tie<sup>1</sup>, Xueyang Zhou<sup>1</sup>, Tianhe Gu<sup>1</sup>, Ruihang Zhang<sup>1</sup>, Chaoran Hu<sup>1</sup>, Sizhe Zhang<sup>1</sup>  
- Mengqu Sun<sup>2</sup>, Yan Zhang<sup>1</sup>, Pan Zhou<sup>1</sup>, Lichao Sun<sup>2</sup>  
<sup>1</sup>Huazhong University of Science and Technology  
<sup>2</sup>Lehigh University


