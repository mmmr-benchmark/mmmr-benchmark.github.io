# MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks

![MMMR Logo](static/images/MMMR_logo.ico)

## 🌟 Introduction

**MMMR** is a benchmarking framework designed to evaluate the reasoning capabilities of Multi-Modal Large Language Models (MLLMs) and their thinking-enhanced counterparts (MLLMs-T). It supports a wide range of task types involving multi-modal data (text, image, structured data), with a focus on:

- Logical Reasoning
- Mathematical Reasoning
- Spatio-Temporal Understanding
- Code Comprehension
- Map Navigation
- Scientific Reasoning

Visit the website [here](#) for a detailed overview.

## 📦 Project Structure

.
├── static/
│ ├── css/ # CSS styles (Bulma, FontAwesome, etc.)
│ ├── images/ # Icons, banners, and visual assets
├── index.html # Main webpage


## 🔗 Live Resources

- 🤗 [Dataset on HuggingFace](https://huggingface.co/datasets/csegirl/MMMR)  
- 💻 [Code on GitHub](https://github.com/CsEgir/MLRM-Bench/tree/master)  
<!-- - 📄 [Paper on arXiv](https://arxiv.org/abs/<ARXIV_ID>) -->

## 🧪 Highlights

- 📊 Bar charts comparing different MLLMs
- 🧠 Thinking Quality Assessment using RTQ, RTA, RSC, Acc, TLen, and OA
- 🧩 Insight synthesis to guide model improvement

## 👨‍💻 Authors

- Guiyao Tie<sup>1</sup>, Xueyang Zhou<sup>1</sup>, Tianhe Gu<sup>1</sup>, Ruihang Zhang<sup>1</sup>, Chaoran Hu<sup>1</sup>, Sizhe Zhang<sup>1</sup>  
- Mengqu Sun<sup>2</sup>, Yan Zhang<sup>1</sup>, Pan Zhou<sup>1</sup>, Lichao Sun<sup>2</sup>  
<sup>1</sup>Huazhong University of Science and Technology  
<sup>2</sup>Lehigh University


